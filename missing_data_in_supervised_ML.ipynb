{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>Missing data in supervised ML</center>\n",
    "### <center>Andras Zsom</center>\n",
    "<center>Lead Data Scientist and Adjunct Lecturer in Data Science</center>\n",
    "<center>Center for Computation and Visualization</center>\n",
    "<center>Brown University</center>\n",
    "\n",
    "https://github.com/brown-ccv/ODSC_East_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About me\n",
    "- Born and raised in Hungary\n",
    "- Astrophysics PhD at MPIA, Heidelberg, Germany\n",
    "- Postdoctoral researcher at MIT (still in astrophysics at the time)\n",
    "- Started at Brown in December 2015 as a Data Scientist\n",
    "- Promoted to Lead Data Scientist in 2017\n",
    "- Adjunct Lecturer in Data Science this semester\n",
    "   - Teaching the course *DATA1030: Hands-on data science* to the DS master students at Brown\n",
    "   \n",
    "https://github.com/brown-ccv/ODSC_East_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Science at Brown\n",
    "- Center for Computation and Visualization\n",
    "- Institutional Data group\n",
    "   - Data-driven decision support and predictive modeling for Brownâ€™s administrative units\n",
    "   - Academic research on data-intensive projects\n",
    "- **OPEN POSITION** - more on this later\n",
    "\n",
    "https://github.com/brown-ccv/ODSC_East_2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will be able to\n",
    "- Describe the three main types of missingness patterns\n",
    "- Evaluate simple approaches for handling missing values\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply multivariate imputation\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before we start, a few words on our dataset: kaggle house price\n",
    "- good for educational purposes\n",
    "   - messy data that requires quite a bit of preprocessing\n",
    "   - a nice mixture of continuous, ordinal, and categorical features, each feature type has missing values\n",
    "- lots of excellent kernels on kaggle\n",
    "   - check them out [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
    "- dataset and description available in repo\n",
    "   - let's take a look!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing values often occur in datasets\n",
    "- survey data: not everyone answers all the questions\n",
    "- medical data: not all tests/treatments/etc are performed on all patients\n",
    "- sensor can be offline or malfunctioning\n",
    "- customer data: not every user uses all features of an app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Missing values are an issue for multiple reasons\n",
    "\n",
    "#### Concenptual reason\n",
    "- missing values can introduce biases\n",
    "    - bias: the samples (the data points) are not representative of the underlying distribution/population\n",
    "    - any conclusion drawn from a biased dataset is also biased.\n",
    "    - rich people tend to not fill out survey questions about their salaries and the mean salary estimated from survey data tend to be lower than true value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Practical reason\n",
    "- missing values (NaN, NA, inf) are incompatible with sklearn\n",
    "   - all values in an array need to be numerical otherwise sklearn will throw a *ValueError*\n",
    "- there are a few supervised ML techniques that work with missing values (e.g., XGBoost, CatBoost)\n",
    "   - we will cover those later today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- **Describe the three main types of missingness patterns**\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Missing data patterns\n",
    "\n",
    "- **MCAR** - Missing Complete At Random\n",
    "   - some people skip some survey questions by accident\n",
    "- **MAR** - Missing At Random\n",
    "   - males are less likely to fill out a survey on depression\n",
    "   - this has nothing to do with their level of depression after accounting for maleness\n",
    "- **MNAR** - Missing Not At Random\n",
    "   - depressed people are less likely to fill out a survey on depression due to their level of depression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCAR test\n",
    "\n",
    "- MCAR can be diagnosed with a statistical test ([Little, 1988](https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478722))\n",
    "   - python implementation available in the [pymice](https://github.com/RianneSchouten/pymice) package or in the skipped slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# from the pymice package \n",
    "# https://github.com/RianneSchouten/pymice\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as ma\n",
    "import scipy.stats as st\n",
    "\n",
    "def checks_input_mcar_tests(data):\n",
    "    \"\"\" Checks whether the input parameter of class McarTests is correct\n",
    "            Parameters\n",
    "            ----------\n",
    "            data:\n",
    "                The input of McarTests specified as 'data'\n",
    "            Returns\n",
    "            -------\n",
    "            bool\n",
    "                True if input is correct\n",
    "            \"\"\"\n",
    "\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        print(\"Error: Data should be a Pandas DataFrame\")\n",
    "        return False\n",
    "\n",
    "    if not any(data.dtypes.values == np.float):\n",
    "        if not any(data.dtypes.values == np.int):\n",
    "            print(\"Error: Dataset cannot contain other value types than floats and/or integers\")\n",
    "            return False\n",
    "\n",
    "    if not data.isnull().values.any():\n",
    "        print(\"Error: No NaN's in given data\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def mcar_test(data):\n",
    "    \"\"\" Implementation of Little's MCAR test\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Pandas DataFrame\n",
    "        An incomplete dataset with samples as index and variables as columns\n",
    "    Returns\n",
    "    -------\n",
    "    p_value: Float\n",
    "        This value is the outcome of a chi-square statistical test, testing whether the null hypothesis\n",
    "        'the missingness mechanism of the incomplete dataset is MCAR' can be rejected.\n",
    "    \"\"\"\n",
    "\n",
    "    if not checks_input_mcar_tests(data):\n",
    "        raise Exception(\"Input not correct\")\n",
    "\n",
    "    dataset = data.copy()\n",
    "    vars = dataset.dtypes.index.values\n",
    "    n_var = dataset.shape[1]\n",
    "\n",
    "    # mean and covariance estimates\n",
    "    # ideally, this is done with a maximum likelihood estimator\n",
    "    gmean = dataset.mean()\n",
    "    gcov = dataset.cov()\n",
    "\n",
    "    # set up missing data patterns\n",
    "    r = 1 * dataset.isnull()\n",
    "    mdp = np.dot(r, list(map(lambda x: ma.pow(2, x), range(n_var))))\n",
    "    sorted_mdp = sorted(np.unique(mdp))\n",
    "    n_pat = len(sorted_mdp)\n",
    "    correct_mdp = list(map(lambda x: sorted_mdp.index(x), mdp))\n",
    "    dataset['mdp'] = pd.Series(correct_mdp, index=dataset.index)\n",
    "\n",
    "    # calculate statistic and df\n",
    "    pj = 0\n",
    "    d2 = 0\n",
    "    for i in range(n_pat):\n",
    "        dataset_temp = dataset.loc[dataset['mdp'] == i, vars]\n",
    "        select_vars = ~dataset_temp.isnull().any()\n",
    "        pj += np.sum(select_vars)\n",
    "        select_vars = vars[select_vars]\n",
    "        means = dataset_temp[select_vars].mean() - gmean[select_vars]\n",
    "        select_cov = gcov.loc[select_vars, select_vars]\n",
    "        mj = len(dataset_temp)\n",
    "        parta = np.dot(means.T, np.linalg.solve(select_cov, np.identity(select_cov.shape[1])))\n",
    "        d2 += mj * (np.dot(parta, means))\n",
    "\n",
    "    df = pj - n_var\n",
    "\n",
    "    # perform test and save output\n",
    "    p_value = 1 - st.chi2.cdf(d2, df)\n",
    "\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Takeaway\n",
    "- it can be challenging to infer the missingness pattern from an incomplete dataset\n",
    "   - There is a statistical test to differentiate MCAR and MAR\n",
    "   - MNAR is difficult/impossible to diagnose to the best of my knowledge\n",
    "- multiple patterns can be present in the data\n",
    "   - even worse, multiple patterns can be present in one feature!\n",
    "   - missing values in a feature can occur due to a mix of MCAR, MAR, MNAR \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- **Evaluate simple approaches for handling missing values**\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple approaches for handling missing values\n",
    "- 1) categorical/ordinal features: treat missing values as another category\n",
    "    - missing values in categorical/ordinal features are not a big deal\n",
    "- 2) continuous features: this is the tough part\n",
    "    - sklearn's SimpleImputer\n",
    "- 3) exclude points or features with missing values\n",
    "    - might be OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1a) Missing values in a categorical feature\n",
    "- YAY - this is not an issue at all!\n",
    "- Categorical feature needs to be one-hot encoded anyway\n",
    "- Just replace the missing values with 'NA' or 'missing' and treat it as a separate category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1b) Missing values in a ordinal feature\n",
    "- this can be a bit trickier but usually fine\n",
    "- Ordinal encoder is applied to ordinal features\n",
    "    - where does 'NA' or 'missing' fit into the order of the categories?\n",
    "    - usually first or last\n",
    "- if you can figure this out, you are golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n"
     ]
    }
   ],
   "source": [
    "# read the data\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's load the data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# drop the ID\n",
    "df.drop(columns=['Id'],inplace=True)\n",
    "\n",
    "# the target variable\n",
    "y = df['SalePrice']\n",
    "df.drop(columns=['SalePrice'],inplace=True)\n",
    "# the unprocessed feature matrix\n",
    "X = df.values\n",
    "print(X.shape)\n",
    "# the feature names\n",
    "ftrs = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 79)\n",
      "(292, 79)\n",
      "(292, 79)\n"
     ]
    }
   ],
   "source": [
    "# let's split to train, test, and holdout\n",
    "X_other, X_holdout, y_other, y_holdout = train_test_split(df, y, test_size=0.2, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_other, y_other, test_size=0.25, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# collect the various features\n",
    "cat_ftrs = ['MSZoning','Street','Alley','LandContour','LotConfig','Neighborhood','Condition1','Condition2',\\\n",
    "            'BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Foundation',\\\n",
    "           'Heating','CentralAir','Electrical','GarageType','PavedDrive','MiscFeature','SaleType','SaleCondition']\n",
    "ordinal_ftrs = ['LotShape','Utilities','LandSlope','ExterQual','ExterCond','BsmtQual','BsmtCond','BsmtExposure',\\\n",
    "               'BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','Functional','FireplaceQu','GarageFinish',\\\n",
    "               'GarageQual','GarageCond','PoolQC','Fence']\n",
    "ordinal_cats = [['Reg','IR1','IR2','IR3'],['AllPub','NoSewr','NoSeWa','ELO'],['Gtl','Mod','Sev'],\\\n",
    "               ['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Po','Fa','TA','Gd','Ex'],['NA','No','Mn','Av','Gd'],['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\\\n",
    "               ['NA','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],['Po','Fa','TA','Gd','Ex'],['Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],['NA','Po','Fa','TA','Gd','Ex'],\\\n",
    "               ['NA','Unf','RFn','Fin'],['NA','Po','Fa','TA','Gd','Ex'],['NA','Po','Fa','TA','Gd','Ex'],\n",
    "               ['NA','Fa','TA','Gd','Ex'],['NA','MnWw','GdWo','MnPrv','GdPrv']]\n",
    "num_ftrs = ['MSSubClass','LotFrontage','LotArea','OverallQual','OverallCond','YearBuilt','YearRemodAdd',\\\n",
    "             'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF',\\\n",
    "             'LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr',\\\n",
    "             'KitchenAbvGr','TotRmsAbvGrd','Fireplaces','GarageYrBlt','GarageCars','GarageArea','WoodDeckSF',\\\n",
    "             'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# preprocess with pipeline and columntransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# one-hot encoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant',fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'))])\n",
    "\n",
    "# ordinal encoder\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer2', SimpleImputer(strategy='constant',fill_value='NA')),\n",
    "    ('ordinal', OrdinalEncoder(categories = ordinal_cats))])\n",
    "\n",
    "# standard scaler\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# collect the transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_ftrs),\n",
    "        ('cat', categorical_transformer, cat_ftrs),\n",
    "        ('ord', ordinal_transformer, ordinal_ftrs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(292, 221)\n",
      "(292, 221)\n"
     ]
    }
   ],
   "source": [
    "# fit_transform the training set\n",
    "X_prep = preprocessor.fit_transform(X_train)\n",
    "# little hacky, but collect feature names\n",
    "feature_names = preprocessor.transformers_[0][-1] + \\\n",
    "                list(preprocessor.named_transformers_['cat'][1].get_feature_names(cat_ftrs)) + \\\n",
    "                preprocessor.transformers_[2][-1]\n",
    "\n",
    "df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "print(df_train.shape)\n",
    "\n",
    "# transform the test\n",
    "df_test = preprocessor.transform(X_test)\n",
    "df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "print(df_test.shape)\n",
    "\n",
    "# transform  the holdout\n",
    "df_holdout = preprocessor.transform(X_holdout)\n",
    "df_holdout = pd.DataFrame(data=df_holdout,columns = feature_names)\n",
    "print(df_holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2) Continuous features: mean or median imputation\n",
    "- Imputation means you infer the missing values from the known part of the data\n",
    "- sklearn's SimpleImputer can do mean and median imputation\n",
    "- USUALLY A BAD IDEA!\n",
    "   - MCAR: mean/median of non-missing values is the same as the mean/median of the true underlying distribution, but the variances are different\n",
    "   - not MCAR: the mean/median and the variance of the completed dataset will be off\n",
    "   - supervised ML model is too confident (MCAR) or systematically off (not MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3) Exclude points or features with missing values\n",
    "\n",
    "- easy to do with pandas\n",
    "- it is an ACCEPTABLE approach under two conditions:\n",
    "    - Little's test supports MCAR (p > 0.05)\n",
    "    - only small fraction of points contain missing values (maybe a few percent?)\n",
    "    - the missing values are limited to one or a few features and a large fraction of points are missing from those features (maybe up to 90%?)\n",
    "- if the MCAR assumption is justified, dropping points will not introduce biases to your model\n",
    "- due to the smaller sample size, the confidence of your model might suffer. \n",
    "- what will you do with missing values when you deploy the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions: (876, 221)\n",
      "the p value of the mcar test: 0.029160269814447304\n",
      "fraction of missing values in features:\n",
      "LotFrontage    0.173516\n",
      "MasVnrArea     0.004566\n",
      "GarageYrBlt    0.050228\n",
      "dtype: float64\n",
      "fraction of points with missing values: 0.2237442922374429\n"
     ]
    }
   ],
   "source": [
    "print('data dimensions:',df_train.shape)\n",
    "print('the p value of the mcar test:',mcar_test(df_train))\n",
    "perc_missing_per_ftr = df_train.isnull().sum(axis=0)/df_train.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(df_train.isnull().sum(axis=1)!=0)/df_train.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876, 221)\n",
      "(680, 221)\n",
      "(876, 218)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "# by default, rows/points are dropped\n",
    "df_r = df_train.dropna()\n",
    "print(df_r.shape)\n",
    "# drop features with missing values\n",
    "df_c = df_train.dropna(axis=1)\n",
    "print(df_c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- **Apply XGBoost to a dataset with missing values**\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XGBoost and missing values\n",
    "- sklearn raises an error if the feature matrix (X) contains nans. \n",
    "- XGBoost doesn't! \n",
    "- If a feature with missing values is split:\n",
    "    - XGBoost tries to put the points with missing values to the left and right\n",
    "    - calculates the impurity measure for both options\n",
    "    - puts the points with missing values to the side with the lower impurity\n",
    "- if missingness correlates with the target variable, XGBoost extracts this info!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test RMSE: 24356.306641\n",
      "the holdout RMSE: 33267.239641714295\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "param_grid = {\"learning_rate\": [0.03],\n",
    "              \"n_estimators\": [2000],\n",
    "              \"seed\": [0],\n",
    "              #\"n_jobs\": [6],\n",
    "              #\"reg_alpha\": [0e0,0.1,0.31622777,1.,3.16227766,10.],\n",
    "              #\"reg_lambda\": [0e0,0.1,0.31622777,1.,3.16227766,10.],\n",
    "              \"missing\": [np.nan], \n",
    "              #\"max_depth\": [1,2,3,4,5],\n",
    "              \"colsample_bytree\": [0.9],              \n",
    "              \"subsample\": [0.66]}\n",
    "\n",
    "XGB = xgboost.XGBRegressor()\n",
    "XGB.set_params(**ParameterGrid(param_grid)[0])\n",
    "XGB.fit(df_train,y_train,early_stopping_rounds=50,eval_set=[(df_test, y_test)], verbose=False)\n",
    "print('the test RMSE:',XGB.evals_result()['validation_0']['rmse'][-1])\n",
    "y_holdout_pred = XGB.predict(df_holdout)\n",
    "print('the holdout RMSE:',np.sqrt(mean_squared_error(y_holdout,y_holdout_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- **Apply multivariate imputation**\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Imputation\n",
    "\n",
    "- models each feature with missing values as a function of other features, and uses that estimate for imputation\n",
    "   - at each step, a feature column is designated as target variable y and the other feature columns are treated as feature matrix X\n",
    "   - a regressor is trained on (X, y) for known y\n",
    "   - then, the regressor is used to predict the missing values of y\n",
    "- in the ML pipeline:\n",
    "   - create n imputed datasets\n",
    "   - run all of them through the ML pipeline\n",
    "   - generate n holdout scores\n",
    "   - the uncertainty in the holdout scores is due to the uncertainty in imputation\n",
    "- works on MCAR and MAR, fails on MNAR\n",
    "- paper [here](https://www.jstatsoft.org/article/view/v045i03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# sklearn's IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LotFrontage  MasVnrArea  GarageYrBlt\n",
      "0     0.424926   -0.573303     0.979398\n",
      "1          NaN    0.492835     1.018748\n",
      "2          NaN   -0.573303     0.192399\n",
      "3    -0.049970    0.810076    -0.476551\n",
      "4    -1.474659   -0.022031     0.979398\n",
      "   LotFrontage  MasVnrArea  GarageYrBlt\n",
      "0     0.424926   -0.573303     0.979398\n",
      "1    -1.258797    0.492835     1.018748\n",
      "2    -0.516232   -0.573303     0.192399\n",
      "3    -0.049970    0.810076    -0.476551\n",
      "4    -1.474659   -0.022031     0.979398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/sklearn/impute/_iterative.py:599: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "print(df_train[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n",
    "\n",
    "imputer = IterativeImputer(estimator = RandomForestRegressor(n_estimators=10), random_state=0)\n",
    "X_impute = imputer.fit_transform(df_train)\n",
    "df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "print(df_train_imp[['LotFrontage','MasVnrArea','GarageYrBlt']].head())\n",
    "\n",
    "df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)\n",
    "df_holdout_imp = pd.DataFrame(data=imputer.transform(df_holdout), columns = df_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/datasci_v0.0.2_local4.yml/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test RMSE: 24731.107422\n",
      "the holdout RMSE: 34163.64520140612\n"
     ]
    }
   ],
   "source": [
    "XGB.fit(df_train_imp,y_train,early_stopping_rounds=50,eval_set=[(df_test_imp, y_test)], verbose=False)\n",
    "print('the test RMSE:',XGB.evals_result()['validation_0']['rmse'][-1])\n",
    "y_holdout_pred = XGB.predict(df_holdout_imp)\n",
    "print('the holdout RMSE:',np.sqrt(mean_squared_error(y_holdout,y_holdout_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- **Apply the reduced-features model (also called the pattern submodel approach)**\n",
    "- <font color='LIGHTGRAY'>Decide which approach is best for your dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reduced-features model (or pattern submodel approach)\n",
    "- first described in 2007 in a [JMLR article](http://www.jmlr.org/papers/v8/saar-tsechansky07a.html) as the reduced features model\n",
    "- in 2018, \"rediscovered\" as the pattern submodel approach in [Biostatistics](https://www.ncbi.nlm.nih.gov/pubmed/30203058)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>My holdout set:</center>\n",
    "\n",
    "| index \t| feature 1 \t| feature 2 \t| feature 3 \t| target var \t|\n",
    "|-------\t|:---------:\t|:---------:\t|:---------:\t|:----------:\t|\n",
    "| 0     \t|     <font color='red'>NA</font>    \t|     45    \t|     <font color='red'>NA</font>    \t|      0     \t|\n",
    "| 1     \t|     <font color='red'>NA</font>    \t|     <font color='red'>NA</font>    \t|     8     \t|      1     \t|\n",
    "| 2     \t|     12    \t|     6     \t|     34    \t|      0     \t|\n",
    "| 3     \t|     1     \t|     89    \t|     <font color='red'>NA</font>    \t|      0     \t|\n",
    "| 4     \t|     0     \t|     <font color='red'>NA</font>    \t|     47    \t|      1     \t|\n",
    "| 5     \t|    687    \t|     24    \t|     67    \t|      1     \t|\n",
    "| 6     \t|     <font color='red'>NA</font>    \t|     23    \t|     <font color='red'>NA</font>    \t|      1     \t|\n",
    "\n",
    "To predict points 0 and 6, I will use train and test points that are complete in feature 2.\n",
    "\n",
    "To predict point 1, I will use train and test points that are complete in feature 3.\n",
    "\n",
    "To predict point 2 and 5, I will use train and test points that are complete in features 1-3.\n",
    "\n",
    "Etc. We will train as many models as the number of patterns in holdout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to determine the patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n",
      "[False False False] 223\n",
      "[False False  True] 21\n",
      "[False  True False] 1\n",
      "[ True False False] 44\n",
      "[ True False  True] 2\n",
      "[ True  True False] 1\n"
     ]
    }
   ],
   "source": [
    "mask = df_holdout[['LotFrontage','MasVnrArea','GarageYrBlt']].isnull()\n",
    "unique_rows, counts = np.unique(mask, axis=0,return_counts=True)\n",
    "print(unique_rows.shape) # 6 patterns, we will train 6 models\n",
    "for i in range(len(counts)):\n",
    "    print(unique_rows[i],counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def xgb_model(X_train, Y_train, X_test, Y_test, X_holdout, Y_holdout, verbose=1):\n",
    "\n",
    "    # make into row vectors to avoid an obnoxious sklearn/xgb warning\n",
    "    Y_train = np.reshape(np.array(Y_train), (1, -1)).ravel()\n",
    "    Y_test = np.reshape(np.array(Y_test), (1, -1)).ravel()\n",
    "    Y_holdout = np.reshape(np.array(Y_holdout), (1, -1)).ravel()\n",
    "\n",
    "    XGB = xgboost.XGBRegressor(n_jobs=1)\n",
    "    \n",
    "    # find the best parameter set\n",
    "    param_grid = {\"learning_rate\": [0.03],\n",
    "                  \"n_estimators\": [2000],\n",
    "                  \"seed\": [0],\n",
    "                  #\"n_jobs\": [6],\n",
    "                  #\"reg_alpha\": [0e0,0.1,0.31622777,1.,3.16227766,10.],\n",
    "                  #\"reg_lambda\": [0e0,0.1,0.31622777,1.,3.16227766,10.],\n",
    "                  \"missing\": [np.nan], \n",
    "                  #\"max_depth\": [1,2,3,4,5],\n",
    "                  \"colsample_bytree\": [0.9],              \n",
    "                  \"subsample\": [0.66]}\n",
    "\n",
    "    pg = ParameterGrid(param_grid)\n",
    "\n",
    "    scores = np.zeros(len(pg))\n",
    "\n",
    "    for i in range(len(pg)):\n",
    "        if verbose >= 5:\n",
    "            print(\"Param set \" + str(i + 1) + \" / \" + str(len(pg)))\n",
    "        params = pg[i]\n",
    "        XGB.set_params(**params)\n",
    "        eval_set = [(X_test, Y_test)]\n",
    "        XGB.fit(X_train, Y_train,\n",
    "                early_stopping_rounds=50, eval_set=eval_set, verbose=False)# with early stopping\n",
    "        Y_test_pred = XGB.predict(X_test, ntree_limit=XGB.best_ntree_limit)\n",
    "        scores[i] = mean_squared_error(Y_test,Y_test_pred)\n",
    "\n",
    "    best_params = np.array(pg)[scores == np.max(scores)]\n",
    "    if verbose >= 4:\n",
    "        print('Test set max score and best parameters are:')\n",
    "        print(np.max(scores))\n",
    "        print(best_params)\n",
    "\n",
    "    # test the model on the holdout set with best parameter set\n",
    "    XGB.set_params(**best_params[0])\n",
    "    XGB.fit(X_train,Y_train,\n",
    "            early_stopping_rounds=50,eval_set=eval_set, verbose=False)\n",
    "    Y_holdout_pred = XGB.predict(X_holdout, ntree_limit=XGB.best_ntree_limit)\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print ('The MSE is:',mean_squared_error(Y_holdout,Y_holdout_pred))\n",
    "    if verbose >= 2:\n",
    "        print ('The predictions are:')\n",
    "        print (Y_holdout_pred)\n",
    "    if verbose >= 3:\n",
    "        print(\"Feature importances:\")\n",
    "        print(XGB.feature_importances_)\n",
    "\n",
    "    return (mean_squared_error(Y_holdout,Y_holdout_pred), Y_holdout_pred, XGB.feature_importances_)\n",
    "\n",
    "# Function: Reduced-feature XGB model\n",
    "# all the inputs need to be pandas DataFrame\n",
    "def reduced_feature_xgb(X_train, Y_train, X_test, Y_test, X_holdout, Y_holdout):\n",
    "    \n",
    "    # find all unique patterns of missing value in holdout set\n",
    "    mask = X_holdout.isnull()\n",
    "    unique_rows = np.array(np.unique(mask, axis=0))\n",
    "    all_Y_holdout_pred = pd.DataFrame()\n",
    "    \n",
    "    print('there are', len(unique_rows), 'unique missing value patterns.')\n",
    "    \n",
    "    # divide holdout sets into subgroups according to the unique patterns\n",
    "    for i in range(len(unique_rows)):\n",
    "        print ('working on unique pattern', i)\n",
    "        ## generate X_holdout subset that matches the unique pattern i\n",
    "        sub_X_holdout = pd.DataFrame()\n",
    "        sub_Y_holdout = pd.Series()\n",
    "        for j in range(len(mask)): # check each row in mask\n",
    "            row_mask = np.array(mask.iloc[j])\n",
    "            if np.array_equal(row_mask, unique_rows[i]): # if the pattern matches the ith unique pattern\n",
    "                sub_X_holdout = sub_X_holdout.append(X_holdout.iloc[j])# append the according X_holdout row j to the subset\n",
    "                sub_Y_holdout = sub_Y_holdout.append(Y_holdout.iloc[[j]])# append the according Y_holdout row j\n",
    "        sub_X_holdout = sub_X_holdout[X_holdout.columns[~unique_rows[i]]]\n",
    "        \n",
    "        ## choose the according reduced features for subgroups\n",
    "        sub_X_train = pd.DataFrame()\n",
    "        sub_Y_train = pd.DataFrame()\n",
    "        sub_X_test = pd.DataFrame()\n",
    "        sub_Y_test = pd.DataFrame()\n",
    "        # 1.cut the feature columns that have nans in the according sub_X_holdout\n",
    "        sub_X_train = X_train[X_train.columns[~unique_rows[i]]]\n",
    "        sub_X_test = X_test[X_test.columns[~unique_rows[i]]]\n",
    "        # 2.cut the rows in the sub_X_train and sub_X_test that have any nans\n",
    "        sub_X_train = sub_X_train.dropna()\n",
    "        sub_X_test = sub_X_test.dropna()   \n",
    "        # 3.cut the sub_Y_train and sub_Y_test accordingly\n",
    "        sub_Y_train = Y_train.iloc[sub_X_train.index]\n",
    "        sub_Y_test = Y_test.iloc[sub_X_test.index]\n",
    "        \n",
    "        # run XGB\n",
    "        sub_Y_holdout_pred = xgb_model(sub_X_train, sub_Y_train, sub_X_test, \n",
    "                                       sub_Y_test, sub_X_holdout, sub_Y_holdout, verbose=0)\n",
    "        sub_Y_holdout_pred = pd.DataFrame(sub_Y_holdout_pred[1],columns=['sub_Y_holdout_pred'],\n",
    "                                          index=sub_Y_holdout.index)\n",
    "        print('   RMSE:',np.sqrt(mean_squared_error(sub_Y_holdout,sub_Y_holdout_pred)))\n",
    "        # collect the holdout predictions\n",
    "        all_Y_holdout_pred = all_Y_holdout_pred.append(sub_Y_holdout_pred)\n",
    "        \n",
    "    # rank the final Y_holdout_pred according to original Y_holdout index\n",
    "    all_Y_holdout_pred = all_Y_holdout_pred.sort_index()\n",
    "    Y_holdout = Y_holdout.sort_index()\n",
    "               \n",
    "    # get global RMSE\n",
    "    total_RMSE = np.sqrt(mean_squared_error(Y_holdout,all_Y_holdout_pred))\n",
    "    \n",
    "    return total_RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A python implementation is available on the skipped slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 6 unique missing value patterns.\n",
      "working on unique pattern 0\n",
      "   RMSE: 36882.33649761256\n",
      "working on unique pattern 1\n",
      "   RMSE: 14122.835714181436\n",
      "working on unique pattern 2\n",
      "   RMSE: 7912.15625\n",
      "working on unique pattern 3\n",
      "   RMSE: 19059.686269128022\n",
      "working on unique pattern 4\n",
      "   RMSE: 20818.556654658543\n",
      "working on unique pattern 5\n",
      "   RMSE: 55023.453125\n",
      "final RMSE: 33488.799390845474\n"
     ]
    }
   ],
   "source": [
    "print('final RMSE:',reduced_feature_xgb(df_train, y_train, df_test, y_test, df_holdout, y_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color='LIGHTGRAY'>Learning Objectives</font>\n",
    "\n",
    "<font color='LIGHTGRAY'>By the end of this workshop, you will be able to</font>\n",
    "- <font color='LIGHTGRAY'>Describe the three main types of missingness patterns</font>\n",
    "- <font color='LIGHTGRAY'>Evaluate simple approaches for handling missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply XGBoost to a dataset with missing values</font>\n",
    "- <font color='LIGHTGRAY'>Apply multivariate imputation</font>\n",
    "- <font color='LIGHTGRAY'>Apply the reduced-features model (also called the pattern submodel approach)</font>\n",
    "- **Decide which approach is best for your dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which approach is best for my data?\n",
    "- **XGB**: run $n$ XGB models with $n$ different seeds\n",
    "- **imputation**: prepare $n$ different imputations and run $n$ XGB models on them\n",
    "- **reduced-features**: run $n$ reduced-features model with $n$ different seeds\n",
    "- rank the three methods based on how significantly different the corresponding mean scores are\n",
    "   - I hope to talk about the results of this experiment at ODSC East next year!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now you can\n",
    "- Describe the three main types of missingness patterns\n",
    "- Evaluate simple approaches for handling missing values\n",
    "- Apply XGBoost to a dataset with missing values\n",
    "- Apply multivariate imputation\n",
    "- Apply the reduced-features model (also called the pattern submodel approach)\n",
    "- Decide which approach is best for your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> Thanks for your attention! </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
